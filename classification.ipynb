{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D,Activation,LeakyReLU,BatchNormalization,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple___Apple_scab',\n",
       " 'Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Background_without_leaves',\n",
       " 'Blueberry___healthy',\n",
       " 'Cherry___healthy',\n",
       " 'Cherry___Powdery_mildew',\n",
       " 'Corn___Cercospora_leaf_spot Gray_leaf_spot',\n",
       " 'Corn___Common_rust',\n",
       " 'Corn___healthy',\n",
       " 'Corn___Northern_Leaf_Blight',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___healthy',\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
       " 'Orange___Haunglongbing_(Citrus_greening)',\n",
       " 'Peach___Bacterial_spot',\n",
       " 'Peach___healthy',\n",
       " 'Pepper,_bell___Bacterial_spot',\n",
       " 'Pepper,_bell___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___healthy',\n",
       " 'Potato___Late_blight',\n",
       " 'Raspberry___healthy',\n",
       " 'Soybean___healthy',\n",
       " 'Squash___Powdery_mildew',\n",
       " 'Strawberry___healthy',\n",
       " 'Strawberry___Leaf_scorch',\n",
       " 'Tomato___Bacterial_spot',\n",
       " 'Tomato___Early_blight',\n",
       " 'Tomato___healthy',\n",
       " 'Tomato___Late_blight',\n",
       " 'Tomato___Leaf_Mold',\n",
       " 'Tomato___Septoria_leaf_spot',\n",
       " 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
       " 'Tomato___Target_Spot',\n",
       " 'Tomato___Tomato_mosaic_virus',\n",
       " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = r'D:\\Github\\Plant-Disease-Detection\\Plant_leave_diseases_dataset_without_augmentation'\n",
    "\n",
    "os.chdir(root_dir)\n",
    "listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dimension of resized image\n",
    "DEFAULT_IMAGE_SIZE = tuple((256, 256))\n",
    "\n",
    "# Number of images used to train the model\n",
    "N_IMAGES = 100\n",
    "\n",
    "data_dir = os.path.join(root_dir,'train')\n",
    "\n",
    "\"\"\"We use the function `convert_image_to_array` to resize an image to the size `DEFAULT_IMAGE_SIZE` we defined above.\"\"\"\n",
    "\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None:\n",
    "            image = cv2.resize(image, DEFAULT_IMAGE_SIZE)   \n",
    "            return img_to_array(image)\n",
    "        else:\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Load images from all classes ...\")\n",
    "plant_disease_folder_list = listdir(data_dir)\n",
    "print(len(plant_disease_folder_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_list, label_list = [], []\n",
    "\n",
    "try:\n",
    "    print(\"Loding Image Data ...\")\n",
    "    for s in listdir():\n",
    "        plant_disease_folder_list = listdir(s)\n",
    "\n",
    "        for plant_disease_folder in plant_disease_folder_list:\n",
    "            print(f\"Processing  {plant_disease_folder} ...\")\n",
    "            plant_disease_image_list = listdir(f\"{data_dir}/{plant_disease_folder}/\")\n",
    "\n",
    "            for image in plant_disease_image_list[:N_IMAGES]:\n",
    "                image_directory = f\"{data_dir}/{plant_disease_folder}/{image}\"\n",
    "                if image_directory.endswith(\".jpg\")==True or image_directory.endswith(\".JPG\")==True:\n",
    "                    image_list.append(convert_image_to_array(image_directory))\n",
    "                    label_list.append(plant_disease_folder)\n",
    "\n",
    "    print(\"All the images have successfully loaded!!\")  \n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the loaded training image data into numpy array\n",
    "np_image_list = np.array(image_list, dtype=np.float16) / 255.0\n",
    "\n",
    "# Check the number of images loaded for training\n",
    "image_len = len(image_list)\n",
    "print(f\"Total number of images: {image_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Examine the labels/classes in the training dataset.\"\"\"\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "\n",
    "# pickle.dump(label_binarizer,open('plant_disease_label_transform.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "\n",
    "print(\"Total number of classes: \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Augment and Split Dataset\n",
    "Using `ImageDataGenerator` to augment data by performing various operations on the training images.\n",
    "\"\"\"\n",
    "\n",
    "augment = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                             height_shift_range=0.1, shear_range=0.2, \n",
    "                             zoom_range=0.2, horizontal_flip=True, \n",
    "                             fill_mode=\"nearest\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting the data into training and test sets for validation purpose.\"\"\"\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42)\n",
    "print('Successfully split data into TRAIN & TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Build Model\n",
    "Defining the hyperparameters of the plant disease classification model.\n",
    "\"\"\"\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPS = 100\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a Sequential Model to build CNN for multi-class classification\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "inputShape = (HEIGHT, WIDTH, DEPTH)\n",
    "chanDim = -1\n",
    "\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (DEPTH, HEIGHT, WIDTH)\n",
    "    chanDim = 1\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, name = 'my_dense'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'my_dense'\n",
    "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output) \n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Train Model\n",
    "We initialize Adam optimizer with learning rate and decay parameters. \n",
    "Also, we choose the type of loss and metrics for the model and compile it for training.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize optimizer\n",
    "opt = Adam(learning_rate=LR, decay=LR / EPOCHS)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "print(\"Training CNN...\")\n",
    "history = model.fit(augment.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              steps_per_epoch=len(x_train) // BATCH_SIZE,\n",
    "                              epochs=EPOCHS, \n",
    "                              verbose=1)\n",
    "        \n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Evaluate Model\n",
    "Comparing the accuracy and loss by plotting the graph for training and validation.\n",
    "\"\"\"\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Train and validation accuracy\n",
    "plt.plot(epochs, acc, 'b', label='Training accurarcy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accurarcy')\n",
    "plt.title('Training and Validation accurarcy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Train and validation loss\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Evaluating model accuracy by using the `evaluate` method\"\"\"\n",
    "\n",
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "\n",
    "model.save('PDD_completemodel')\n",
    "model.save('PDD_completemodel.h5')\n",
    "intermediate_layer_model.save('PDD_IntermediateModel')\n",
    "intermediate_layer_model.save('PDD_IntermediateModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_predict = intermediate_layer_model.predict(x_train)\n",
    "# print(x_train_predict.shape)\n",
    "\n",
    "x_test_predict = intermediate_layer_model.predict(x_test)\n",
    "print(x_test_predict.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
